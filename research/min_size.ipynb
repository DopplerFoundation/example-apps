{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Given a pretrained model, how small can we make the file that stores the model weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's calculate what our theorectical lower limit. We can do this with a simple calculation:\n",
    "\n",
    "$$ \\text{min file size} = \\text{num of model params} * \\text{size per param} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min file size = 553,430,176 bytes \n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/docs/master/tensors.html\n",
    "PRECISION = {\n",
    "    str(torch.FloatTensor): 32,\n",
    "    str(torch.DoubleTensor): 64,\n",
    "    str(torch.HalfTensor): 16,\n",
    "    str(torch.ByteTensor): 8,\n",
    "    str(torch.CharTensor): 8,\n",
    "    str(torch.ShortTensor): 16,\n",
    "    str(torch.IntTensor): 32,\n",
    "    str(torch.LongTensor): 64,\n",
    "}\n",
    "BITS_PER_BYTE = 8\n",
    "\n",
    "def get_param_count(model):\n",
    "    num_model_params = 0\n",
    "    for p in model.parameters():\n",
    "        num_model_params += p.numel()\n",
    "    return num_model_params\n",
    "\n",
    "def get_param_precision(model):\n",
    "    tensor_type = type(model.parameters().next().data)\n",
    "    return PRECISION[str(tensor_type)]\n",
    "\n",
    "def get_min_file_size(model):\n",
    "    num_model_params = get_param_count(model)\n",
    "    size_per_param = get_param_precision(model) / BITS_PER_BYTE\n",
    "    return num_model_params * size_per_param\n",
    "\n",
    "min_file_size = get_min_file_size(model)\n",
    "print 'min file size = {:,} bytes '.format(min_file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So our lower limit is ~553 MB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How close is this calculation to the value we see when using `torch.save`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual file size = 553,458,637\n",
      "difference = 5.1e-05%\n"
     ]
    }
   ],
   "source": [
    "def get_actual_file_size(model):\n",
    "    file_name = 'model.pt'\n",
    "    torch.save(model, file_name)\n",
    "    return os.path.getsize(file_name)\n",
    "\n",
    "actual_file_size = get_actual_file_size(model)\n",
    "print 'actual file size = {:,}'.format(actual_file_size)\n",
    "print 'difference = {:.2}%'.format(abs(min_file_size - actual_file_size) / float(min_file_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual file size is ~= the lower limit...is this a fluke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /Users/jkarimi91/.torch/models/inception_v3_google-1a9a5a14.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /Users/jkarimi91/.torch/models/vgg13-c768596a.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /Users/jkarimi91/.torch/models/densenet201-c1103571.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /Users/jkarimi91/.torch/models/vgg11-bbd30ac9.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /Users/jkarimi91/.torch/models/resnet101-5d3b4d8f.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\" to /Users/jkarimi91/.torch/models/squeezenet1_0-a815701f.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\" to /Users/jkarimi91/.torch/models/squeezenet1_1-f364aa15.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /Users/jkarimi91/.torch/models/vgg11_bn-6002323d.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /Users/jkarimi91/.torch/models/vgg19_bn-c79401a0.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /Users/jkarimi91/.torch/models/densenet161-8d451a50.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /Users/jkarimi91/.torch/models/resnet152-b121ed2d.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /Users/jkarimi91/.torch/models/densenet169-b2777c0a.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max difference = 0.0021%\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This will take awhile to run especially if it is your first \n",
    "# using one of the pretrained models; pytorch needs to download the weights \n",
    "# if they do not already exist locally.\n",
    "\n",
    "excluded = [models.ResNet, models.VGG, \n",
    "            models.DenseNet, models.AlexNet, \n",
    "            models.SqueezeNet, models.Inception3]\n",
    "\n",
    "# https://stackoverflow.com/questions/21885814/how-to-iterate-through-a-modules-functions\n",
    "max_diff = None\n",
    "for _, method in models.__dict__.iteritems():\n",
    "    # Is it a pretrained model?\n",
    "    if callable(method) and method not in excluded:\n",
    "        model = method(pretrained=True)\n",
    "        diff = abs(get_min_file_size(model) - get_actual_file_size(model)) / float(min_file_size)\n",
    "        max_diff = max(diff, max_diff)\n",
    "        \n",
    "print 'max difference = {:.2}%'.format(max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nope, `torch.save` really is optimal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what does this tell us i.e. what is the answer to our original question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer: Our file sizes are already optimal. In order to further reduce the file size, we would have to use lower precsision weights or a smaller model altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
